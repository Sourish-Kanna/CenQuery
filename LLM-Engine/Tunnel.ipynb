{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeF1uw4ssOkm"
      },
      "outputs": [],
      "source": [
        "# 1. Install Dependencies\n",
        "!pip install -q fastapi uvicorn pyngrok nest_asyncio torch transformers peft bitsandbytes accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbmNw-PasQbo"
      },
      "outputs": [],
      "source": [
        "# --- MEMORY FIX START ---\n",
        "import os\n",
        "# This helps with \"fragmentation\" errors\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "# --- MEMORY FIX END ---\n",
        "\n",
        "# 2. Imports\n",
        "import torch\n",
        "import gc\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import os\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-RYpqkvsCOJ"
      },
      "outputs": [],
      "source": [
        "# 4. The Optimized LLM Engine (Merged Logic)\n",
        "class LLMEngine:\n",
        "    def __init__(self):\n",
        "        # --- CONFIGURATION ---\n",
        "        self.base_model_id = \"defog/llama-3-sqlcoder-8b\"\n",
        "        self.adapter_id = \"Sourish-Kanna/CenQuery\"  # <--- YOUR NEW REPO\n",
        "        # ---------------------\n",
        "\n",
        "        # 1. Clean Memory (Crucial for Colab Free Tier)\n",
        "        print(\"üßπ Cleaning GPU Memory...\")\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"‚è≥ Loading Base Model: {self.base_model_id}...\")\n",
        "\n",
        "        # 2. 4-bit Config (NF4 - High Precision Quantization)\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "        # 3. Load Base Model\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.base_model_id,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            # dtype=torch.float16, # Managed by bnb_config above\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Disable cache to save VRAM (Good for inference, bad for long chat history)\n",
        "        self.base_model.config.use_cache = False\n",
        "\n",
        "        # 4. Load Tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_id)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = \"right\" # Llama-3 standard for generation\n",
        "\n",
        "        print(f\"‚è≥ Downloading Adapter from HF: {self.adapter_id}...\")\n",
        "        self.model = PeftModel.from_pretrained(self.base_model, self.adapter_id, is_trainable=False)\n",
        "        self.model.eval()\n",
        "\n",
        "        # 5. Define Terminators (Crucial for Llama-3 to stop chatting)\n",
        "        self.terminators = [\n",
        "            self.tokenizer.eos_token_id,\n",
        "            self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "        ]\n",
        "\n",
        "        print(\"‚úÖ System Ready! CenQuery Brain is Online.\")\n",
        "\n",
        "    def generate(self, prompt: str):\n",
        "        \"\"\"\n",
        "        Generates SQL and applies robust cleaning to remove hallucinations.\n",
        "        \"\"\"\n",
        "        # --- MEMORY CLEANUP BEFORE GENERATION ---\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # ----------------------------------------\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=300,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=self.terminators, # Use explicit Llama-3 terminators\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                do_sample=False # Deterministic\n",
        "            )\n",
        "\n",
        "        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # --- Robust Cleaning Logic ---\n",
        "        # 1. Remove the Input Prompt\n",
        "        # (We check if prompt exists in output to avoid errors if context window shifts)\n",
        "        if prompt in full_output:\n",
        "            generated_text = full_output.replace(prompt, \"\").strip()\n",
        "        else:\n",
        "            generated_text = full_output # Fallback\n",
        "\n",
        "        # 2. Handle \"### SQL\" Header (Style 2 Compliance)\n",
        "        # If the model repeats the header, split on it.\n",
        "        if \"### SQL\" in generated_text:\n",
        "            generated_text = generated_text.split(\"### SQL\")[-1].strip()\n",
        "\n",
        "        # 3. Stop Hallucinations (Road trips, explanations, etc.)\n",
        "        # Stop at \"assistant\" header or double newlines if no SQL found\n",
        "        clean_sql = generated_text.split(\"assistant\")[0].split(\"<|start_header_id|>\")[0].strip()\n",
        "\n",
        "        # If there are multiple queries, take the first one\n",
        "        if \";\" in clean_sql:\n",
        "            clean_sql = clean_sql.split(\";\")[0] + \";\"\n",
        "\n",
        "        return clean_sql\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AB_BMST9L3f"
      },
      "outputs": [],
      "source": [
        "# Initialize App & Engine\n",
        "app = FastAPI(title=\"CenQuery LLM Service (Optimized)\")\n",
        "nest_asyncio.apply()\n",
        "engine = LLMEngine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvLRlhSm8nsz"
      },
      "outputs": [],
      "source": [
        "# 5. API Endpoint\n",
        "class QueryRequest(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "async def generate_sql(req: QueryRequest):\n",
        "    try:\n",
        "        print(\"Request Start\")\n",
        "        print(req)\n",
        "        print(f\"üî∏ Prompt: [{req.prompt}]\")\n",
        "        sql = engine.generate(req.prompt)\n",
        "        print(f\"üîπ Generated: {sql}\") # Log for debugging in Colab\n",
        "        return {\"sql\": sql}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\") # Log for debugging in Colab\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/\", include_in_schema=False)\n",
        "async def root():\n",
        "    return {\"message\": \"CenQuery LLM Service (Optimized) is Online!\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QRQDGcSsXqE"
      },
      "outputs": [],
      "source": [
        "# 6. Start Ngrok\n",
        "# PASTE YOUR NGROK TOKEN BELOW (Get it from dashboard.ngrok.com)\n",
        "# NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
        "from google.colab import userdata\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK')\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"üöÄ Service A is Online at: {public_url}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDAe49NpsXdS"
      },
      "outputs": [],
      "source": [
        "# 7. Run Server\n",
        "# Apply the patch (Good practice, even if using await)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configure the server\n",
        "config = uvicorn.Config(app, port=8000)\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "# Start the server in the existing loop\n",
        "# This cell will stay \"Busy\" [*] - that is normal!\n",
        "await server.serve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak_dlfzkUtYO"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# 1. Kill all running tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "print(\"üõë ngrok tunnels killed. You can now restart the server.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
