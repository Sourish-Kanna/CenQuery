{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "EVAL_DATASET_PATH = \"Dataset/eval_data/eval_(Mixed).jsonl\" # Change to your actual 350 eval file path\n",
    "BASE_MODEL_ID = \"defog/llama-3-sqlcoder-8b\"\n",
    "ADAPTER_ID = \"Sourish-Kanna/CenQuery\"\n",
    "# ---------------------\n",
    "\n",
    "def load_model():\n",
    "    print(\"‚è≥ Loading Base Model and Adapter for Evaluation...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_ID, device_map=\"auto\", quantization_config=bnb_config\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, ADAPTER_ID, is_trainable=False)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate():\n",
    "    model, tokenizer = load_model()\n",
    "    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    \n",
    "    with open(EVAL_DATASET_PATH, 'r') as f:\n",
    "        eval_data = [json.loads(line) for line in f]\n",
    "\n",
    "    print(f\"üìã Found {len(eval_data)} questions for evaluation.\")\n",
    "    \n",
    "    exact_matches = 0\n",
    "    total = len(eval_data)\n",
    "    \n",
    "    # Loop through the dataset with a progress bar\n",
    "    for item in tqdm(eval_data, desc=\"Evaluating\"):\n",
    "        prompt = item['prompt'] # Adjust key if your JSONL uses a different key for the prompt\n",
    "        ground_truth_sql = item['completion'].strip().lower() # Adjust key for target SQL\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, max_new_tokens=300, eos_token_id=terminators, \n",
    "                pad_token_id=tokenizer.eos_token_id, do_sample=False\n",
    "            )\n",
    "            \n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Clean the generated SQL\n",
    "        generated_sql = full_output.replace(prompt, \"\").strip()\n",
    "        if \"### SQL\" in generated_sql:\n",
    "            generated_sql = generated_sql.split(\"### SQL\")[-1].strip()\n",
    "        generated_sql = generated_sql.split(\"assistant\")[0].split(\"<|start_header_id|>\")[0].strip().lower()\n",
    "        if \";\" in generated_sql:\n",
    "            generated_sql = generated_sql.split(\";\")[0] + \";\"\n",
    "            \n",
    "        # Check Exact Match\n",
    "        if generated_sql == ground_truth_sql:\n",
    "            exact_matches += 1\n",
    "\n",
    "    exact_match_accuracy = (exact_matches / total) * 100\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"üéØ EVALUATION COMPLETE\")\n",
    "    print(f\"Total Questions: {total}\")\n",
    "    print(f\"Exact Match Accuracy: {exact_match_accuracy:.2f}%\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
